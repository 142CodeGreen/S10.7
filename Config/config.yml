models:
  - type: main
    engine: nvidia_ai_endpoints
    model: meta/llama-3.1-8b-instruct  #NVIDIA NIM selection

instructions:
  - type: general
    content: |
      Below is a conversation between a user and this chatbot based on the uploaded documents.
      The bot is designed to answer questions based on the loaded documents.
      The bot is only knowledgeable about loaded documents.
      If the bot does not know the answer to a question, it truthfully says it does not know.

sample_conversation: |
  user "Hi there. Can you help me anser a few questions about the loaded documents?"
    express greeting and ask for assistance
  bot express greeting and confirm and offer assistance
    "Hi there! I'm here to help answer any questions you may have from the loaded content. What would you like to know?"
  user "Can you summarise the loaded docuements?"
    ask about the summary of the documents
  bot respond with a sumary of the documents
    "The loaded documents are about ...."
  user "thanks"
    express appreciation
  bot express appreciation and offer additional help
    "You're welcome. If you have any more questions or if there's anything else I can help you with, please don't hesitate to ask."

knowledge_base:
  type: kb_folder
  path: ./Config/kb


rails:
  # Input rails are invoked when new input from the user is received.
  input:
    flows:
      - self check input
    
  # Output rails are triggered after a bot message has been generated.
  output:
    flows:
      - self check hallucination
      - self check output

actions:
  - type: custom
    name: self_check_hallucination
    return_type: ActionResult

  # Whether to try to use a single LLM call for generating the user intent, next step and bot message.
  dialog:
    single_call:
      enabled: True
      # If a single call fails, whether to fall back to multiple LLM calls.
      fallback_to_multiple_calls: True
    
